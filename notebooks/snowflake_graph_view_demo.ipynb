{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune Analytics with Snowflake Data via Athena Federated Query\n",
    "\n",
    "This notebook demonstrates how to connect PaySim data stored in Snowflake to Neptune Analytics using Athena Federated Query. We will:\n",
    "1. Upload PaySim data to Snowflake and create a table\n",
    "2. Set up AWS Secrets Manager for Snowflake credentials\n",
    "3. Deploy Athena Snowflake connector Lambda function\n",
    "4. Configure Athena data source for Snowflake\n",
    "5. Import data from Snowflake into Neptune Analytics\n",
    "6. Run Louvain algorithm for fraud detection\n",
    "7. Export enriched graph data back to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the Python version:\n",
    "import sys\n",
    "assert sys.version_info >= (3, 11), \"Python 3.11 or higher is required\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import kagglehub\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from nx_neptune.session_manager import SessionManager"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout\n",
    ")\n",
    "for logger_name in [\n",
    "    'nx_neptune.instance_management',\n",
    "    'nx_neptune.session_manager',\n",
    "]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up environment variables for Snowflake connection and AWS resources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def check_env_vars(var_names):\n",
    "    values = {}\n",
    "    for var_name in var_names:\n",
    "        value = os.getenv(var_name)\n",
    "        if not value:\n",
    "            print(f\"Warning: Environment Variable {var_name} is not defined\")\n",
    "            print(f\"You can set it using: %env {var_name}=your-value\")\n",
    "        else:\n",
    "            print(f\"Using {var_name}: {value}\")\n",
    "        values[var_name] = value\n",
    "    return values\n",
    "    \n",
    "env_vars = check_env_vars([\n",
    "    'NETWORKX_S3_IMPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_EXPORT_BUCKET_PATH',\n",
    "    'SNOWFLAKE_ACCOUNT',\n",
    "    'SNOWFLAKE_USER',\n",
    "    'SNOWFLAKE_PASSWORD',\n",
    "    'SNOWFLAKE_DATABASE',\n",
    "    'SNOWFLAKE_SCHEMA',\n",
    "    'SNOWFLAKE_WAREHOUSE',\n",
    "    'SNOWFLAKE_SECRET_ARN',\n",
    "    'ATHENA_SPILL_BUCKET',\n",
    "    'ATHENA_CATALOG_NAME',\n",
    "])\n",
    "\n",
    "s3_location_import = os.getenv('NETWORKX_S3_IMPORT_BUCKET_PATH')\n",
    "s3_location_export = os.getenv('NETWORKX_S3_EXPORT_BUCKET_PATH')\n",
    "snowflake_account = os.getenv('SNOWFLAKE_ACCOUNT')\n",
    "snowflake_user = os.getenv('SNOWFLAKE_USER')\n",
    "snowflake_password = os.getenv('SNOWFLAKE_PASSWORD')\n",
    "snowflake_database = os.getenv('SNOWFLAKE_DATABASE', 'FRAUD_DETECTION')\n",
    "snowflake_schema = os.getenv('SNOWFLAKE_SCHEMA', 'PUBLIC')\n",
    "snowflake_warehouse = os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH')\n",
    "snowflake_secret_arn = os.getenv('SNOWFLAKE_SECRET_ARN')\n",
    "athena_spill_bucket = os.getenv('ATHENA_SPILL_BUCKET')\n",
    "athena_catalog_name = os.getenv('ATHENA_CATALOG_NAME', 'snowflake_catalog')\n",
    "session_name = \"nx-snowflake-demo\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload PaySim Data to Snowflake\n",
    "\n",
    "Download PaySim dataset and upload to Snowflake. You'll need the Snowflake Python connector installed:\n",
    "```bash\n",
    "pip install snowflake-connector-python\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    account=snowflake_account,\n",
    "    warehouse=snowflake_warehouse,\n",
    "    database=snowflake_database,\n",
    "    schema=snowflake_schema\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create database and schema if they don't exist\n",
    "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {snowflake_database}\")\n",
    "cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {snowflake_database}.{snowflake_schema}\")\n",
    "cursor.execute(f\"USE SCHEMA {snowflake_database}.{snowflake_schema}\")\n",
    "\n",
    "# Create PaySim transactions table\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS TRANSACTIONS (\n",
    "   STEP INT,\n",
    "   TYPE VARCHAR(50),\n",
    "   AMOUNT FLOAT,\n",
    "   NAMEORIG VARCHAR(100),\n",
    "   OLDBALANCEORG FLOAT,\n",
    "   NEWBALANCEORIG FLOAT,\n",
    "   NAMEDEST VARCHAR(100),\n",
    "   OLDBALANCEDEST FLOAT,\n",
    "   NEWBALANCEDEST FLOAT,\n",
    "   ISFRAUD INT,\n",
    "   ISFLAGGEDFRAUD INT\n",
    "   ) \\\n",
    "\"\"\"\n",
    "cursor.execute(create_table_sql)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Download PaySim dataset\n",
    "paysim_path = Path(kagglehub.dataset_download(\"ealaxi/paysim1\"))\n",
    "print(\"Path to paysim dataset files:\", paysim_path)\n",
    "\n",
    "# Upload CSV file to Snowflake stage and load into table\n",
    "csv_file = next(paysim_path.glob('*.csv'))\n",
    "cursor.execute(\"CREATE OR REPLACE STAGE paysim_stage\")\n",
    "cursor.execute(f\"PUT file://{csv_file} @paysim_stage\")\n",
    "cursor.execute(\"\"\"\n",
    "COPY INTO TRANSACTIONS\n",
    "FROM @paysim_stage\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_DELIMITER = ',' SKIP_HEADER = 1)\n",
    "ON_ERROR = 'CONTINUE'\n",
    "\"\"\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify data loaded\n",
    "cursor.execute(\"SELECT COUNT(*) FROM TRANSACTIONS\")\n",
    "count = cursor.fetchone()[0]\n",
    "print(f\"Found {count} transactions in Snowflake\")\n",
    "\n",
    "# close connection\n",
    "cursor.close()\n",
    "conn.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Up AWS Secrets Manager for Snowflake Credentials\n",
    "\n",
    "Create a secret in AWS Secrets Manager to store Snowflake connection credentials.\n",
    "\n",
    "**Create the secret using AWS CLI:**\n",
    "```bash\n",
    "aws secretsmanager create-secret \\\n",
    "    --name snowflake-credentials \\\n",
    "    --secret-string '{\"username\":\"YOUR_SNOWFLAKE_USER\",\"password\":\"YOUR_SNOWFLAKE_PASSWORD\"}'\n",
    "```\n",
    "\n",
    "**Or via AWS Console:**\n",
    "1. Go to AWS Secrets Manager → Secrets → Store a new secret\n",
    "2. Select \"Other type of secret\"\n",
    "3. Add key-value pairs:\n",
    "   - Key: `username`, Value: Your Snowflake username\n",
    "   - Key: `password`, Value: Your Snowflake password\n",
    "4. Name: `snowflake-credentials`\n",
    "5. Create secret and copy the ARN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify Secrets Manager secret exists\n",
    "secrets_client = boto3.client('secretsmanager')\n",
    "\n",
    "if snowflake_secret_arn:\n",
    "    try:\n",
    "        response = secrets_client.describe_secret(SecretId=snowflake_secret_arn)\n",
    "        print(f\"Secret found: {response['Name']}\")\n",
    "        print(f\"ARN: {response['ARN']}\")\n",
    "    except secrets_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"ERROR: Secret not found: {snowflake_secret_arn}\")\n",
    "        print(\"Please create the secret as described above\")\n",
    "else:\n",
    "    print(\"WARNING: SNOWFLAKE_SECRET_ARN not set\")\n",
    "    print(\"Set it using: %env SNOWFLAKE_SECRET_ARN=arn:aws:secretsmanager:region:account:secret:name\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy Athena Snowflake Connector Lambda\n",
    "\n",
    "Deploy the Athena Snowflake connector from AWS Serverless Application Repository.\n",
    "\n",
    "**Manual Setup via AWS Console:**\n",
    "\n",
    "1. Go to **AWS Serverless Application Repository**\n",
    "2. Search for \"AthenaSnowflakeConnector\"\n",
    "3. Click on the application and **Deploy**\n",
    "4. Configure parameters:\n",
    "   - **Application name**: `athena-snowflake-connector`\n",
    "   - **SpillBucket**: S3 bucket for large query results (e.g., `my-athena-spill-bucket`)\n",
    "   - **SecretNamePrefix**: `snowflake-credentials` (your secret name)\n",
    "   - **SnowflakeConnectionString**: `snowflake://<account>.snowflakecomputing.com/?warehouse=<warehouse>&db=<database>&schema=<schema>`\n",
    "5. Acknowledge IAM capabilities and **Deploy**\n",
    "6. Wait for CloudFormation stack to complete (5-10 minutes)\n",
    "7. Copy the Lambda function ARN from the stack outputs\n",
    "\n",
    "**Alternative: Deploy via AWS CLI:**\n",
    "```bash\n",
    "# Create spill bucket if needed\n",
    "aws s3 mb s3://my-athena-spill-bucket\n",
    "\n",
    "# Deploy the connector\n",
    "aws serverlessrepo create-cloud-formation-change-set \\\n",
    "    --application-id arn:aws:serverlessrepo:us-east-1:292517598671:applications/AthenaSnowflakeConnector \\\n",
    "    --stack-name athena-snowflake-connector \\\n",
    "    --capabilities CAPABILITY_IAM \\\n",
    "    --parameter-overrides \\\n",
    "        SpillBucket=my-athena-spill-bucket \\\n",
    "        SecretNamePrefix=snowflake-credentials \\\n",
    "        SnowflakeConnectionString=\"snowflake://<account>.snowflakecomputing.com/?warehouse=<warehouse>&db=<database>&schema=<schema>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify Lambda function exists\n",
    "lambda_client = boto3.client('lambda')\n",
    "lambda_function_name = 'athena-snowflake-connector'\n",
    "\n",
    "try:\n",
    "    response = lambda_client.get_function(FunctionName=lambda_function_name)\n",
    "    lambda_arn = response['Configuration']['FunctionArn']\n",
    "    print(f\"Lambda function found: {lambda_function_name}\")\n",
    "    print(f\"ARN: {lambda_arn}\")\n",
    "except lambda_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"ERROR: Lambda function '{lambda_function_name}' not found\")\n",
    "    print(\"Please deploy the Athena Snowflake connector as described above\")\n",
    "    lambda_arn = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Athena Data Catalog for Snowflake\n",
    "\n",
    "Create an Athena data catalog that uses the Lambda connector to query Snowflake.\n",
    "\n",
    "**Manual Setup via AWS Console:**\n",
    "\n",
    "1. Go to **Athena Console** → **Data sources** → **Create data source**\n",
    "2. Select **Query a data source** → **Snowflake**\n",
    "3. Configure:\n",
    "   - **Data source name**: `snowflake_catalog`\n",
    "   - **Lambda function**: Select `athena-snowflake-connector`\n",
    "4. Click **Create data source**\n",
    "\n",
    "**Alternative: Create via AWS CLI:**\n",
    "```bash\n",
    "aws athena create-data-catalog \\\n",
    "    --name snowflake_catalog \\\n",
    "    --type LAMBDA \\\n",
    "    --parameters \"function=arn:aws:lambda:REGION:ACCOUNT:function:athena-snowflake-connector\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify Athena catalog exists\n",
    "athena_client = boto3.client('athena')\n",
    "\n",
    "try:\n",
    "    response = athena_client.get_data_catalog(Name=athena_catalog_name)\n",
    "    print(f\"Athena catalog '{athena_catalog_name}' found\")\n",
    "    print(f\"Catalog type: {response['DataCatalog']['Type']}\")\n",
    "    if 'Parameters' in response['DataCatalog']:\n",
    "        print(f\"Lambda function: {response['DataCatalog']['Parameters'].get('function', 'N/A')}\")\n",
    "except athena_client.exceptions.InvalidRequestException:\n",
    "    print(f\"ERROR: Athena catalog '{athena_catalog_name}' not found\")\n",
    "    print(\"Please create the catalog manually as described above\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neptune Analytics Instance\n",
    "\n",
    "Provision a Neptune Analytics instance or retrieve an existing one."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "session = SessionManager.session(session_name)\n",
    "graph_list = session.list_graphs()\n",
    "print(\"Available graphs:\")\n",
    "for g in graph_list:\n",
    "    print(g)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "graph = await session.get_or_create_graph(config={\"provisionedMemory\": 32})\n",
    "print(f\"Retrieved graph: {graph}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data from Snowflake via Athena Federated Query\n",
    "\n",
    "Query Snowflake data through Athena using the Lambda connector and import into Neptune Analytics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# SQL queries to project nodes and edges from Snowflake\n",
    "SOURCE_AND_DESTINATION_CUSTOMERS = f\"\"\"\n",
    "SELECT DISTINCT \"~id\", 'customer' AS \"~label\"\n",
    "FROM (\n",
    "     SELECT NAMEORIG as \"~id\"\n",
    "     FROM \"{athena_catalog_name}\".\"{snowflake_database}\".\"{snowflake_schema}\".TRANSACTIONS\n",
    "     WHERE NAMEORIG IS NOT NULL\n",
    "     UNION ALL\n",
    "     SELECT NAMEDEST as \"~id\"\n",
    "     FROM \"{athena_catalog_name}\".\"{snowflake_database}\".\"{snowflake_schema}\".TRANSACTIONS\n",
    "     WHERE NAMEDEST IS NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "BANK_TRANSACTIONS = f\"\"\"\n",
    "SELECT\n",
    "    NAMEORIG as \"~from\",\n",
    "    NAMEDEST as \"~to\",\n",
    "    TYPE AS \"~label\",\n",
    "    STEP AS \"step:Int\",\n",
    "    AMOUNT AS \"amount:Float\",\n",
    "    OLDBALANCEORG AS \"oldbalanceOrg:Float\",\n",
    "    NEWBALANCEORIG AS \"newbalanceOrig:Float\",\n",
    "    OLDBALANCEDEST AS \"oldbalanceDest:Float\",\n",
    "    NEWBALANCEDEST AS \"newbalanceDest:Float\",\n",
    "    ISFRAUD AS \"isFraud:Int\",\n",
    "    ISFLAGGEDFRAUD AS \"isFlaggedFraud:Int\"\n",
    "FROM \"{athena_catalog_name}\".\"{snowflake_database}\".\"{snowflake_schema}\".TRANSACTIONS\n",
    "WHERE NAMEORIG IS NOT NULL AND NAMEDEST IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "await session.import_from_table(\n",
    "    graph,\n",
    "    s3_location_import,\n",
    "    [SOURCE_AND_DESTINATION_CUSTOMERS, BANK_TRANSACTIONS],\n",
    "    catalog=athena_catalog_name,\n",
    "    database=snowflake_database\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Louvain Algorithm for Fraud Detection\n",
    "\n",
    "Run Louvain community detection to identify potential fraud networks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify graph data\n",
    "all_nodes = graph.execute_query(\"MATCH (n) RETURN n LIMIT 10\")\n",
    "print(f\"Sample nodes: {all_nodes}\")\n",
    "\n",
    "all_edges = graph.execute_query(\"MATCH ()-[r]-() RETURN r LIMIT 10\")\n",
    "print(f\"Sample edges: {all_edges}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run Louvain algorithm and mutate graph with community property\n",
    "louvain_result = graph.execute_query(\n",
    "    'CALL neptune.algo.louvain.mutate({iterationTolerance:1e-07, writeProperty:\"community\"}) '\n",
    "    'YIELD success AS success RETURN success'\n",
    ")\n",
    "print(f\"Louvain result: {louvain_result}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Enriched Graph Data to S3\n",
    "\n",
    "Export the graph with community annotations back to S3 as Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "csv_catalog = 'AwsDataCatalog'\n",
    "csv_database = 'fraud_detection_results'\n",
    "csv_table_name = 'transactions_csv'\n",
    "\n",
    "iceberg_vertices_table_name = 'customers_with_communities'\n",
    "iceberg_edges_table_name = 'transactions_enriched'\n",
    "iceberg_catalog = 's3tablescatalog/fraud-detection-results'\n",
    "iceberg_database = 'fraud_detection_results'\n",
    "\n",
    "await session.export_to_table(\n",
    "    graph[\"id\"],\n",
    "    s3_location_export,\n",
    "    csv_table_name,\n",
    "    csv_catalog,\n",
    "    csv_database,\n",
    "    iceberg_vertices_table_name,\n",
    "    iceberg_edges_table_name,\n",
    "    iceberg_catalog,\n",
    "    iceberg_database\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "session.destroy_all_graphs()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Snowflake Setup**: Uploaded PaySim data to Snowflake and created a table\n",
    "2. **AWS Secrets Manager**: Stored Snowflake credentials securely\n",
    "3. **Athena Federated Query**: Deployed Lambda connector for Snowflake integration\n",
    "4. **Data Import**: Queried Snowflake via Athena and imported into Neptune Analytics\n",
    "5. **Graph Analytics**: Ran Louvain community detection for fraud pattern identification\n",
    "6. **Data Export**: Exported enriched graph data back to S3 as Iceberg tables\n",
    "\n",
    "This architecture enables graph analytics on Snowflake data without ETL pipelines, combining Snowflake's data warehouse capabilities with Neptune Analytics' graph algorithms through Athena Federated Query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Using NetworkX API\n",
    "\n",
    "You can also run algorithms using the NetworkX API with Neptune backend:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.config.backends.neptune.graph_id = graph.graph_id\n",
    "\n",
    "# Run Louvain with NetworkX API\n",
    "result = nx.community.louvain_communities(\n",
    "    nx.Graph(), \n",
    "    backend=\"neptune\", \n",
    "    write_property=\"community\"\n",
    ")\n",
    "print(f\"Louvain result: {result}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
