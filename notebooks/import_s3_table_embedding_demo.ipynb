{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune Analytics Instance Management With S3 Table Embedding Projections\n",
    "\n",
    "\n",
    "This notebook demonstrates how embedding data stored in a data lake can be imported into Amazon Neptune Analytics and used to leverage the TopK algorithm package. \n",
    "\n",
    "The goal is to ingest embedding vectors as graph data and enable similarity search, allowing the system to identify products with similar characteristics based on their embedding representations.\n",
    "\n",
    "The content of this notebook includes:\n",
    "1. Download and modify the Kaggle fashion dataset, enriching it with an embedding column generated using Amazon Bedrock, and store the result in Amazon S3\n",
    "2. Create an Athena projection from S3 Tables bucket.\n",
    "3. Import the projection into Neptune Analytics.\n",
    "4. Run topK.byNode to search for similar products and return similarity scores\n",
    "\n",
    "\n",
    "1. Create a projection from S3 Tables bucket.\n",
    "2. Import the projection into Neptune Analytics.\n",
    "3. Run Louvain algorithm on the provisioned instance to create communities.\n",
    "4. Export the graph back into S3 Tables bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary libraries and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import dotenv\n",
    "\n",
    "from nx_neptune import empty_s3_bucket, instance_management, NeptuneGraph, set_config_graph_id\n",
    "from nx_neptune.instance_management import _execute_athena_query, _clean_s3_path\n",
    "from nx_neptune.utils.utils import get_stdout_logger, check_env_vars, _get_bedrock_embedding, read_csv, write_csv, \\\n",
    "    push_to_s3\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from nx_neptune.session_manager import SessionManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Check for environment variables necessary for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to see detailed information about the instance creation process\n",
    "logger = get_stdout_logger(__name__, [\n",
    "    'nx_neptune.instance_management',\n",
    "    'nx_neptune.utils.task_future',\n",
    "    'nx_neptune.session_manager',\n",
    "    'nx_neptune.interface',\n",
    "    __name__\n",
    "])\n",
    "\n",
    "# Check for optional environment variables\n",
    "env_vars = check_env_vars([\n",
    "    'NETWORKX_S3_IMPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_EXPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_TABLES_CATALOG',\n",
    "    'NETWORKX_S3_TABLES_DATABASE',\n",
    "    'NETWORKX_S3_TABLES_TABLENAME',\n",
    "])\n",
    "\n",
    "# Get environment variables\n",
    "s3_location_import = os.getenv('NETWORKX_S3_IMPORT_BUCKET_PATH')\n",
    "s3_location_export = os.getenv('NETWORKX_S3_EXPORT_BUCKET_PATH')\n",
    "s3_tables_database = os.getenv('NETWORKX_S3_TABLES_DATABASE')\n",
    "s3_tables_tablename = os.getenv('NETWORKX_S3_TABLES_TABLENAME')\n",
    "graph_id = os.getenv('NETWORKX_GRAPH_ID')\n",
    "session_name = \"nx-athena-test-full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "PaySim data is available from [kaggle](https://www.kaggle.com/code/kartik2112/fraud-detection-on-paysim-dataset/input?select=PS_20174392719_1491204439457_log.csv).\n",
    "\n",
    "Data should be uploaded to an S3 bucket, and an athena table created for that bucket.\n",
    "\n",
    "The PaySim dataset includes a simulated mobile money dataset, that involves transactions between client actors and banks. We can use this dataset to detect fraudulent activities in the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the fahsion.csv from Kaggle dataset (Only the style.csv).\n",
    "# https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small\n",
    "\n",
    "def append_embedding(headers, rows):\n",
    "    # Inject header\n",
    "    fieldnames = headers + [\"embedding\"]\n",
    "    # Inject embedding\n",
    "    bedrock = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "    # Generate vector embeddings.\n",
    "    for row in rows:\n",
    "        # embedding =  [1.1] * 384\n",
    "        embedding = _get_bedrock_embedding(bedrock,\n",
    "                                           row[\"masterCategory\"] +\n",
    "                                           row[\"subCategory\"] +\n",
    "                                           row[\"articleType\"] +\n",
    "                                           row[\"baseColour\"])[0]\n",
    "        row[\"embedding\"] = \";\".join(map(str, embedding))\n",
    "\n",
    "    return fieldnames, rows\n",
    "\n",
    "\n",
    "data_path = \"../example/resources/styles.csv\"\n",
    "data_w_embedding_path = \"../example/resources/styles_embedding.csv\"\n",
    "\n",
    "output_bucket = \"s3://ak-athena-result/\"\n",
    "data_bucket = \"s3://ak-athena-import/\"\n",
    "log_bucket = \"s3://ak-athena-log/\"\n",
    "\n",
    "\n",
    "athena_client = boto3.client('athena')\n",
    "\n",
    "# Read data from data path\n",
    "headers, rows = read_csv(data_path, 10)\n",
    "# Add the embedding\n",
    "headers, rows = append_embedding(headers, rows)\n",
    "# Write to new csv\n",
    "write_csv(data_w_embedding_path, headers, rows)\n",
    "\n",
    "# Push to s3\n",
    "empty_s3_bucket(data_bucket)\n",
    "push_to_s3(data_w_embedding_path, _clean_s3_path(data_bucket),\"styles_embedding.csv\")\n",
    "\n",
    "\n",
    "print(\"Completed data preparation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Athena related work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create external data\n",
    "create_csv_table_stmt = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {s3_tables_tablename} (\n",
    "    `id` int,\n",
    "    `gender` string,\n",
    "    `masterCategory` string,\n",
    "    `subCategory` string,\n",
    "    `articleType` string,\n",
    "    `baseColour` string,\n",
    "    `season` string,\n",
    "    `year` int,\n",
    "    `usage` string,\n",
    "    `productDisplayname` string,\n",
    "    `embedding` array<float>\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',', 'collection.delim' = ';')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '{data_bucket}'\n",
    "TBLPROPERTIES ('classification' = 'csv', 'skip.header.line.count'='1');\n",
    "\"\"\"\n",
    "\n",
    "_execute_athena_query(athena_client, create_csv_table_stmt, log_bucket, database=s3_tables_database)\n",
    "\n",
    "empty_s3_bucket(output_bucket)\n",
    "\n",
    "# Projection\n",
    "create_csv_table_stmt = f\"\"\"\n",
    "    SELECT\n",
    "        \"id\" AS \"~id\",\n",
    "        \"masterCategory\" AS \"~label\",\n",
    "        \"baseColour\" AS \"baseColour\",\n",
    "        array_join(\n",
    "            transform(embedding, x -> cast(x AS varchar)), ';'\n",
    "        ) AS \"embedding:vector\"\n",
    "    FROM {s3_tables_tablename};\n",
    "\"\"\"\n",
    "\n",
    "_execute_athena_query(athena_client, create_csv_table_stmt, output_bucket, database=s3_tables_database)\n",
    "\n",
    "\n",
    "empty_s3_bucket(output_bucket, file_extension=\".csv.metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data from S3\n",
    "\n",
    "Import data from S3 into the Neptune Analytics graph and wait for the operation to complete. <br>\n",
    "IAM permisisons required for import: <br>\n",
    " - s3:GetObject, kms:Decrypt, kms:GenerateDataKey, kms:DescribeKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = await instance_management.import_csv_from_s3(\n",
    "        NeptuneGraph.from_config(set_config_graph_id(graph_id)),\n",
    "        output_bucket,\n",
    "        reset_graph_ahead=True,\n",
    "        skip_snapshot=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK_QUERY = \"\"\"\n",
    "    MATCH (n) WHERE id(n) = '30805'\n",
    "    CALL neptune.algo.vectors.topK.byNode(\n",
    "      n, {topK: 3})\n",
    "    YIELD node, score\n",
    "    RETURN node, score\n",
    "\"\"\"\n",
    "\n",
    "config = set_config_graph_id(graph_id)\n",
    "na_graph = NeptuneGraph.from_config(config)\n",
    "all_nodes = na_graph.execute_call(TOPK_QUERY)\n",
    "for n in all_nodes:\n",
    "    print(n[\"node\"][\"~id\"] + \", score:\" + str(n[\"score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete lifecycle of running analytics from a datalake projection into Neptune Analytics instance:\n",
    "\n",
    "1. **Creation**: We created a new Neptune Analytics instance on demand\n",
    "2. **Import**: We imported a projection of the datalake\n",
    "3. **Usage**: We ran graph algorithms (Louvain) on the instance and mutated the data\n",
    "4. **Deletion**: We exported the updated data back into the datalake into an iceberg table\n",
    "\n",
    "The session manager (`SessionManager`) provides an easy mechanism to execute general datalake functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
