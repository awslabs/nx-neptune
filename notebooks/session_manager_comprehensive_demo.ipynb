{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive SessionManager Demo\n",
    "\n",
    "This notebook demonstrates comprehensive Neptune Analytics instance management using SessionManager with automatic cleanup.\n",
    "\n",
    "This notebook covers:\n",
    "1. Start/stop instances\n",
    "2. Reset instance\n",
    "3. Export an instance to S3\n",
    "4. Create a snapshot from an instance\n",
    "5. Create multiple instances in parallel (empty, from S3, from snapshot)\n",
    "6. Automatic cleanup using context manager (__exit__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary libraries and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import logging\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from nx_neptune.session_manager import SessionManager, CleanupTask\n",
    "from nx_neptune import instance_management\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "\n",
    "Set up required environment variables for S3 operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Required environment variables\n",
    "s3_export_bucket = os.getenv('NETWORKX_S3_EXPORT_BUCKET_PATH')\n",
    "s3_import_bucket = os.getenv('NETWORKX_S3_IMPORT_BUCKET_PATH')\n",
    "\n",
    "print(f\"S3 Export Bucket: {s3_export_bucket}\")\n",
    "print(f\"S3 Import Bucket: {s3_import_bucket}\")\n",
    "\n",
    "\n",
    "if not all([s3_export_bucket, s3_import_bucket]):\n",
    "    print(\"⚠️ Warning: Some environment variables are not set. S3 operations may fail.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Start/Stop Instances\n",
    "\n",
    "Create an instance, stop it, then start it again using SessionManager."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Scenario 1: Start/Stop Instances ===\")\n",
    "\n",
    "with SessionManager(\"start-stop-demo\", cleanup_task=CleanupTask.DESTROY) as start_stop_session:\n",
    "    # Create a new instance\n",
    "    print(\"Creating new instance...\")\n",
    "    graph = await start_stop_session.get_or_create_graph()\n",
    "    graph_name = graph.name\n",
    "    print(f\"Created instance: {graph_name}\")\n",
    "    \n",
    "    # Stop the instance\n",
    "    print(f\"Stopping instance...{graph_name}\")\n",
    "    await start_stop_session.stop_graph(graph_name)\n",
    "    print(\"Instance stopped\")\n",
    "    \n",
    "    # Start the instance again\n",
    "    print(f\"Starting instance...{graph_name}\")\n",
    "    await start_stop_session.start_graph(graph_name)\n",
    "    print(\"Instance started\")\n",
    "    \n",
    "    # List graphs to verify status\n",
    "    graphs = start_stop_session.list_graphs()\n",
    "    print(f\"Current graphs: {len(graphs)}\")\n",
    "    for graph in graphs:\n",
    "        print(f\"  - {graph.name}: {graph.status}\")\n",
    "\n",
    "print(\"✅ Scenario 1 completed - instances automatically destroyed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Reset Instance\n",
    "\n",
    "Create an instance, add some data, then reset it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Scenario 2: Reset Instance ===\")\n",
    "\n",
    "with SessionManager(\"reset-demo\", cleanup_task=CleanupTask.DESTROY) as reset_session:\n",
    "    # Create a new instance\n",
    "    print(\"Creating new instance...\")\n",
    "    graph = await reset_session.get_or_create_graph()\n",
    "    graph_name = graph.name\n",
    "    print(f\"Created instance: {graph_name}\")\n",
    "    \n",
    "    # Reset the instance\n",
    "    print(f\"Resetting instance...{graph_name}\")\n",
    "    await reset_session.reset_graph(graph_name)\n",
    "    print(\"Instance reset completed\")\n",
    "\n",
    "print(\"✅ Scenario 2 completed - instances automatically destroyed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3: Export Instance to S3\n",
    "\n",
    "Create an instance with data and export it to S3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Scenario 3: Export Instance to S3 ===\")\n",
    "\n",
    "if s3_export_bucket:\n",
    "    with SessionManager(\"export-demo\", cleanup_task=CleanupTask.DESTROY) as export_session:\n",
    "        # Create a new instance\n",
    "        print(\"Creating new instance...\")\n",
    "        graph = await export_session.get_or_create_graph()\n",
    "        graph_name = graph.name\n",
    "        graph_id = graph.id\n",
    "        print(f\"Created instance: {graph_name}\")\n",
    "\n",
    "        # add data\n",
    "        opencypher = \"CREATE (g:City {name: 'Glasgow'})-[r:IN]->(s:Country {name: 'Scotland'}) RETURN g, r, s\"\n",
    "        export_session.execute_query(graph_id, opencypher)\n",
    "\n",
    "        # export graph to S3 bucket as CSV\n",
    "        print(f\"Exporting graph to S3 at {s3_export_bucket}...\")\n",
    "        task_id = await export_session.export_to_csv(graph, s3_export_bucket)\n",
    "        print(\"Exporting graph completed\")\n",
    "\n",
    "        import_location = f\"{s3_export_bucket}{task_id}/\"\n",
    "\n",
    "        # import graph from S3 bucket\n",
    "        async def import_csv():\n",
    "            print(f\"Import graph from S3 at {import_location} to {graph_name}...\")\n",
    "            task_id = await export_session.import_from_csv(graph, import_location, reset_graph_ahead=True)\n",
    "            print(\"Importing graph completed\")\n",
    "            return task_id\n",
    "\n",
    "        # create new graph from S3 bucket\n",
    "        async def create_from_import():\n",
    "            print(f\"Import graph from S3 at {import_location} to new graph...\")\n",
    "            import_graph = await export_session.create_from_csv(import_location)\n",
    "            print(\"Create graph completed\")\n",
    "            return import_graph\n",
    "\n",
    "        results = await asyncio.gather(\n",
    "            import_csv(),\n",
    "            create_from_import(),\n",
    "            return_exceptions=True\n",
    "        )\n",
    "\n",
    "        print(f\"Parallel creation completed.\")\n",
    "\n",
    "        # TODO Remove s3 export\n",
    "\n",
    "        # List all graphs\n",
    "        graphs = export_session.list_graphs()\n",
    "        print(f\"Total graphs in session: {len(graphs)}\")\n",
    "        for graph in graphs:\n",
    "            print(f\"  - {graph.name}: {graph.status}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping S3 export - NETWORKX_S3_EXPORT_BUCKET_PATH not set\")\n",
    "\n",
    "print(\"✅ Scenario 3 completed - instances automatically destroyed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: Create Snapshot from Instance\n",
    "\n",
    "Create an instance, add data, create a snapshot, then use the snapshot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Scenario 4: Create Snapshot from Instance ===\")\n",
    "\n",
    "with SessionManager(\"snapshot-demo\", cleanup_task=CleanupTask.DESTROY) as snapshot_session:\n",
    "    # Create a new instance\n",
    "    print(\"Creating new instance...\")\n",
    "    graph = await snapshot_session.get_or_create_graph()\n",
    "    graph_name = graph.name\n",
    "    print(f\"Created instance: {graph_name}\")\n",
    "\n",
    "    # TODO - add create snapshot calls to session manager\n",
    "    print(\"Creating new snapshot...\")\n",
    "    snapshot_id = await snapshot_session.create_snapshot(graph, \"snapshot-demo\")\n",
    "    print(f\"Created snapshot: {snapshot_id}\")\n",
    "\n",
    "    # TODO - add create instance from snapshot calls to session manager\n",
    "    print(\"Creating new instance from snapshot...\")\n",
    "    snapshot_graph = await snapshot_session.create_from_snapshot(snapshot_id)\n",
    "    snapshot_graph_id = snapshot_graph.name\n",
    "    print(f\"Created instance: {snapshot_graph_id}\")\n",
    "\n",
    "    # TODO - add destroy snapshot calls to session manager\n",
    "    print(\"Deleting snapshot...\")\n",
    "    await snapshot_session.delete_snapshot(snapshot_id)\n",
    "    print(f\"Snapshot delete complete\")\n",
    "\n",
    "print(\"✅ Scenario 4 completed - instances automatically destroyed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5: Create Multiple Instances in Parallel\n",
    "\n",
    "Create three instances in parallel: empty, from S3, and from snapshot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Scenario 5: Create Multiple Instances in Parallel ===\")\n",
    "\n",
    "with SessionManager(\"parallel-demo\", cleanup_task=CleanupTask.DESTROY) as parallel_session:\n",
    "    # First, create a source instance and snapshot for the third parallel creation\n",
    "    print(\"Preparing snapshot for parallel creation...\")\n",
    "    graph = await parallel_session.get_or_create_graph()\n",
    "    graph_name = graph.name\n",
    "    prep_graph_id = graph.id\n",
    "    print(f\"Created instance: {graph_name}\")\n",
    "    \n",
    "    # Create snapshot\n",
    "    snapshot_id = await instance_management.create_graph_snapshot(prep_graph_id, \"parallel-demo-snapshot\")\n",
    "    print(f\"Created snapshot: {snapshot_id}\")\n",
    "    \n",
    "    # Clean up prep instance\n",
    "    await instance_management.delete_na_instance(prep_graph_id)\n",
    "    \n",
    "    # Now create three instances in parallel\n",
    "    print(\"Creating three instances in parallel...\")\n",
    "    \n",
    "    async def create_empty_instance():\n",
    "        graph_id = await instance_management.create_na_instance()\n",
    "        print(f\"Empty instance created: {graph_id}\")\n",
    "        return graph_id\n",
    "    \n",
    "    async def create_s3_instance():\n",
    "        if s3_import_bucket:\n",
    "            graph_id, task_id = await instance_management.create_na_instance_with_s3_import(s3_import_bucket)\n",
    "            print(f\"S3 instance created: {graph_id} (task: {task_id})\")\n",
    "            return graph_id\n",
    "        else:\n",
    "            print(\"⚠️ Skipping S3 instance - NETWORKX_S3_IMPORT_BUCKET_PATH not set\")\n",
    "            return None\n",
    "    \n",
    "    async def create_snapshot_instance():\n",
    "        graph_id = await instance_management.create_na_instance_from_snapshot(snapshot_id)\n",
    "        print(f\"Snapshot instance created: {graph_id}\")\n",
    "        return graph_id\n",
    "    \n",
    "    # Create all instances in parallel\n",
    "    results = await asyncio.gather(\n",
    "        create_empty_instance(),\n",
    "        create_s3_instance(),\n",
    "        create_snapshot_instance(),\n",
    "        return_exceptions=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Parallel creation completed. Created {len([r for r in results if r and not isinstance(r, Exception)])} instances\")\n",
    "    \n",
    "    # List all graphs\n",
    "    graphs = parallel_session.list_graphs()\n",
    "    print(f\"Total graphs in session: {len(graphs)}\")\n",
    "    for graph in graphs:\n",
    "        print(f\"  - {graph.name}: {graph.status}\")\n",
    "    \n",
    "    # Clean up snapshot\n",
    "    print(\"Deleting snapshot...\")\n",
    "    await instance_management.delete_graph_snapshot(snapshot_id)\n",
    "    print(\"Snapshot deleted\")\n",
    "\n",
    "print(\"✅ Scenario 5 completed - all instances automatically destroyed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 6: Automatic Cleanup Demonstration\n",
    "\n",
    "Show how the context manager automatically cleans up resources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=== Scenario 6: Automatic Cleanup Demonstration ===\")\n",
    "\n",
    "# Create a session manager without context manager to show manual cleanup\n",
    "cleanup_session = SessionManager(\"cleanup-demo\", cleanup_task=CleanupTask.DESTROY)\n",
    "\n",
    "try:\n",
    "    # Create some instances\n",
    "    print(\"Creating instances without context manager...\")\n",
    "    graph_id1, graph_id2 = await cleanup_session.create_multiple_instances(2)\n",
    "\n",
    "    graphs = cleanup_session.list_graphs()\n",
    "    print(f\"Created {len(graphs)} instances\")\n",
    "    \n",
    "    # Manually trigger cleanup using __exit__\n",
    "    print(\"Manually triggering cleanup...\")\n",
    "    cleanup_session.__exit__(None, None, None)\n",
    "    \n",
    "    print(\"Manual cleanup completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    # Ensure cleanup even on error\n",
    "    cleanup_session.__exit__(type(e), e, e.__traceback__)\n",
    "\n",
    "print(\"\\n=== Using Context Manager for Automatic Cleanup ===\")\n",
    "\n",
    "# Demonstrate automatic cleanup with context manager\n",
    "with SessionManager(\"auto-cleanup-demo\", cleanup_task=CleanupTask.DESTROY) as auto_cleanup_session:\n",
    "    print(\"Creating instances with context manager...\")\n",
    "    graph_id1, graph_id2 = await auto_cleanup_session.create_multiple_instances(2)\n",
    "\n",
    "    graphs = auto_cleanup_session.list_graphs()\n",
    "    print(f\"Created {len(graphs)} instances\")\n",
    "    print(\"Exiting context manager - automatic cleanup will occur...\")\n",
    "\n",
    "print(\"✅ Scenario 6 completed - automatic cleanup demonstration finished\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive Neptune Analytics instance management using SessionManager:\n",
    "\n",
    "1. **Start/Stop Instances**: Created, stopped, and restarted instances\n",
    "2. **Reset Instance**: Added data and reset an instance to empty state\n",
    "3. **Export to S3**: Exported graph data to S3 bucket\n",
    "4. **Snapshot Operations**: Created snapshots and instances from snapshots\n",
    "5. **Parallel Creation**: Created multiple instances simultaneously\n",
    "6. **Automatic Cleanup**: Used context manager for automatic resource cleanup\n",
    "\n",
    "Key benefits of using SessionManager:\n",
    "- **Automatic Resource Management**: Context manager ensures cleanup\n",
    "- **Session Isolation**: Session names provide resource grouping\n",
    "- **Simplified API**: High-level operations for common tasks\n",
    "- **Error Safety**: Resources cleaned up even on exceptions\n",
    "\n",
    "All instances created during these scenarios were automatically destroyed when exiting the context manager, ensuring no resources were left running."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
