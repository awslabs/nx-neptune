{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune Analytics Instance Management With S3 Table Projections\n",
    "\n",
    "This notebook uses the SessionManager to create projections from S3 Table datalake, load the projection into Neptune Analytics through S3. We will use the Louvain algorithm to find potential fraudulent nodes, and export the mutated graph back into S3 for our datalake.\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Create a projection from S3 Tables bucket.\n",
    "2. Import the projection into Neptune Analytics.\n",
    "3. Run Louvain algorithm on the provisioned instance to create communities.\n",
    "4. Export the graph back into S3 Tables bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary libraries and set up logging."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:22:27.510560Z",
     "start_time": "2026-02-04T23:22:23.948986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "\n",
    "import kagglehub\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from nx_neptune.session_manager import SessionManager"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:25:47.398190Z",
     "start_time": "2026-02-04T23:25:47.396033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure logging to see detailed information about the instance creation process\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout  # Explicitly set output to stdout\n",
    ")\n",
    "# Enable debug logging for the instance management module\n",
    "for logger_name in [\n",
    "    'nx_neptune.instance_management',\n",
    "    'nx_neptune.session_manager',\n",
    "]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Check for environment variables necessary for the notebook."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:25:50.212801Z",
     "start_time": "2026-02-04T23:25:50.207861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def check_env_vars(var_names):\n",
    "    values = {}\n",
    "    for var_name in var_names:\n",
    "        value = os.getenv(var_name)\n",
    "        if not value:\n",
    "            print(f\"Warning: Environment Variable {var_name} is not defined\")\n",
    "            print(f\"You can set it using: %env {var_name}=your-value\")\n",
    "        else:\n",
    "            print(f\"Using {var_name}: {value}\")\n",
    "        values[var_name] = value\n",
    "    return values\n",
    "    \n",
    "# Check for optional environment variables\n",
    "env_vars = check_env_vars([\n",
    "    'NETWORKX_S3_IMPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_EXPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_TABLES_CATALOG',\n",
    "    'NETWORKX_S3_TABLES_DATABASE',\n",
    "    'NETWORKX_S3_TABLES_TABLENAME',\n",
    "])\n",
    "\n",
    "# Get environment variables\n",
    "s3_location_import = os.getenv('NETWORKX_S3_IMPORT_BUCKET_PATH')\n",
    "s3_location_export = os.getenv('NETWORKX_S3_EXPORT_BUCKET_PATH')\n",
    "s3_tables_catalog = os.getenv('NETWORKX_S3_TABLES_CATALOG')\n",
    "s3_tables_database = os.getenv('NETWORKX_S3_TABLES_DATABASE')\n",
    "s3_tables_tablename = os.getenv('NETWORKX_S3_TABLES_TABLENAME')\n",
    "session_name = \"nx-athena-test-full\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NETWORKX_S3_IMPORT_BUCKET_PATH: s3://nx-cit-patents/csv-import/\n",
      "Using NETWORKX_S3_EXPORT_BUCKET_PATH: s3://nx-cit-patents/csv-export/\n",
      "Using NETWORKX_S3_TABLES_CATALOG: s3tablescatalog/nx-fraud-detection-data\n",
      "Using NETWORKX_S3_TABLES_DATABASE: bank_fraud_full\n",
      "Using NETWORKX_S3_TABLES_TABLENAME: transactions\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Setup\n",
    "\n",
    "PaySim data is available from [kaggle](https://www.kaggle.com/code/kartik2112/fraud-detection-on-paysim-dataset/input?select=PS_20174392719_1491204439457_log.csv).\n",
    "\n",
    "Data should be uploaded to an S3 bucket, and an athena table created for that bucket.\n",
    "\n",
    "The PaySim dataset includes a simulated mobile money dataset, that involves transactions between client actors and banks. We can use this dataset to detect fraudulent activities in the simulated data."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:25:54.648349Z",
     "start_time": "2026-02-04T23:25:53.407450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paysim_s3_bucket = 'nx-fraud-detection'\n",
    "paysim_s3_bucket_path = 'data/'\n",
    "\n",
    "# Download the latest version of paysim data\n",
    "paysim_path = Path(kagglehub.dataset_download(\"ealaxi/paysim1\"))\n",
    "\n",
    "print(\"Path to paysim dataset files:\", paysim_path)\n",
    "\n",
    "# upload CSV to an S3 bucket if necessary\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for file_path in paysim_path.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # check if the file already exists\n",
    "        object_list = s3_client.list_objects_v2(\n",
    "            Bucket=paysim_s3_bucket,\n",
    "            Prefix=f\"{paysim_s3_bucket_path}{file_path.name}\"\n",
    "        )\n",
    "        found_keys = object_list[\"KeyCount\"]\n",
    "        print (f\"found {found_keys} matching keys for {paysim_s3_bucket_path}{file_path.name}\")\n",
    "\n",
    "        if found_keys == 0:\n",
    "            print(f\"uploading: {file_path.name} to {paysim_s3_bucket_path}{file_path.name}\")\n",
    "            s3_client.upload_file(\n",
    "                str(file_path),\n",
    "                paysim_s3_bucket,\n",
    "                f\"{paysim_s3_bucket_path}{file_path.name}\"\n",
    "            )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to paysim dataset files: /Users/andrewc/.cache/kagglehub/datasets/ealaxi/paysim1/versions/2\n",
      "INFO - Found credentials in environment variables.\n",
      "found 1 matching keys for data/PS_20174392719_1491204439457_log.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _execute_create_table(stmt, catalog, database, s3_logs_location):\n",
    "    athena_client = boto3.client('athena')\n",
    "\n",
    "    # run athena query and wait for it to complete\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=stmt,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database,\n",
    "            'Catalog': catalog\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_logs_location,\n",
    "        }\n",
    "    )\n",
    "    query_execution_id = response[\"QueryExecutionId\"]\n",
    "    print(f\"Start CREATE TABLE with execution id: {query_execution_id}\")\n",
    "\n",
    "    response_status = 'QUEUED'\n",
    "    while response_status == 'QUEUED' or response_status == 'RUNNING':\n",
    "        sleep(1)\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        response_status = response[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        print(f\"status: {response_status}\")\n",
    "    if response_status == 'SUCCEEDED':\n",
    "        print(\"CREATE TABLE completed\")\n",
    "    else:\n",
    "        print(\"CREATE TABLE failed - please check logs\")\n",
    "\n",
    "# create CSV table from the uploaded data\n",
    "\n",
    "paysim_csv_tablename = 'transactions'\n",
    "paysim_csv_catalog = 'AWSDataCatalog'\n",
    "paysim_csv_database = 'bank_fraud'\n",
    "s3_bucket_location = f\"s3://{paysim_s3_bucket}/{paysim_s3_bucket_path}\"\n",
    "\n",
    "create_csv_table_stmt = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {paysim_csv_tablename} (\n",
    "    `step` int,\n",
    "    `type` string,\n",
    "    `amount` float,\n",
    "    `nameOrig` string,\n",
    "    `oldbalanceOrg` float,\n",
    "    `newbalanceOrig` float,\n",
    "    `nameDest` string,\n",
    "    `oldbalanceDest` float,\n",
    "    `newbalanceDest` float,\n",
    "    `isFraud` int,\n",
    "    `isFlaggedFraud` int\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '{s3_bucket_location}'\n",
    "TBLPROPERTIES ('classification' = 'csv', 'skip.header.line.count'='1');\n",
    "\"\"\"\n",
    "\n",
    "_execute_create_table(create_csv_table_stmt, paysim_csv_catalog, paysim_csv_database, f\"s3://{paysim_s3_bucket}\")\n",
    "\n",
    "# create ICEBERG S3 table from the CSV table\n",
    "\n",
    "create_s3_table_stmt = f\"\"\"\n",
    "CREATE TABLE {s3_tables_tablename}\n",
    "WITH (\n",
    "  table_type = 'ICEBERG',\n",
    "  is_external = false\n",
    ")\n",
    "AS SELECT * FROM \"{paysim_csv_catalog}\".\"{paysim_csv_database}\".\"{paysim_csv_tablename}\"\n",
    "\"\"\"\n",
    "\n",
    "_execute_create_table(create_s3_table_stmt, s3_tables_catalog, s3_tables_database, f\"s3://{paysim_s3_bucket}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a New/Get existing Neptune Analytics Instance\n",
    "\n",
    "Provision a new Neptune Analytics instance on demand, or retrieve an existing neptune-graph. Creating a new instance may take several minutes to complete."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:26:08.462077Z",
     "start_time": "2026-02-04T23:26:07.633364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "session = SessionManager.session(session_name)\n",
    "graph_list = session.list_graphs(with_details=False)\n",
    "print(\"The following graphs are available:\")\n",
    "pprint(graph_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following graphs are available:\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:29:13.483830Z",
     "start_time": "2026-02-04T23:26:10.550637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "session = SessionManager.session(session_name)\n",
    "graph = await session.get_or_create_graph(config={\"provisionedMemory\": 32})\n",
    "print(\"Retrieved graph:\")\n",
    "pprint(graph)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Creating new graph named with prefix: nx-athena-test-full\n",
      "INFO - [2026-02-04 15:26:12] Task [g-ijeab5wbj3] Current status: CREATING\n",
      "INFO - [2026-02-04 15:26:42] Task [g-ijeab5wbj3] Current status: CREATING\n",
      "INFO - [2026-02-04 15:27:12] Task [g-ijeab5wbj3] Current status: CREATING\n",
      "INFO - [2026-02-04 15:27:42] Task [g-ijeab5wbj3] Current status: CREATING\n",
      "INFO - [2026-02-04 15:28:13] Task [g-ijeab5wbj3] Current status: CREATING\n",
      "INFO - [2026-02-04 15:28:43] Task [g-ijeab5wbj3] Current status: CREATING\n",
      "INFO - [2026-02-04 15:29:13] Task [g-ijeab5wbj3] Current status: AVAILABLE\n",
      "INFO - Task [g-ijeab5wbj3] completed at [2026-02-04 15:29:13]\n",
      "Retrieved graph:\n",
      "{'id': 'g-ijeab5wbj3',\n",
      " 'name': 'nx-athena-test-full-80a941fe-91e5-4be3-8df7-b86cb885d38f',\n",
      " 'status': 'AVAILABLE'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data from S3\n",
    "\n",
    "Import data from S3 into the Neptune Analytics graph and wait for the operation to complete. <br>\n",
    "IAM permisisons required for import: <br>\n",
    " - s3:GetObject, kms:Decrypt, kms:GenerateDataKey, kms:DescribeKey"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T00:14:45.141097Z",
     "start_time": "2026-02-05T00:10:38.446326Z"
    }
   },
   "source": [
    "SOURCE_AND_DESTINATION_BANK_CUSTOMERS = f\"\"\"\n",
    "SELECT DISTINCT \"~id\", 'customer' AS \"~label\"\n",
    "FROM (\n",
    "     SELECT \"nameOrig\" as \"~id\"\n",
    "     FROM {s3_tables_tablename}\n",
    "     WHERE \"nameOrig\" IS NOT NULL\n",
    "     UNION ALL\n",
    "     SELECT \"nameDest\" as \"~id\"\n",
    "     FROM {s3_tables_tablename}\n",
    "     WHERE \"nameDest\" IS NOT NULL\n",
    ");\"\"\"\n",
    "\n",
    "BANK_TRANSACTIONS = f\"\"\"\n",
    "SELECT\n",
    "    \"nameOrig\" as \"~from\",\n",
    "    \"nameDest\" as \"~to\",\n",
    "    \"type\" AS \"~label\",\n",
    "    \"step\" AS \"step:Int\",\n",
    "    \"amount\" AS \"amount:Float\",\n",
    "    \"oldbalanceOrg\" AS \"oldbalanceOrg:Float\",\n",
    "    \"newbalanceOrig\" AS \"newbalanceOrig:Float\",\n",
    "    \"oldbalanceDest\" AS \"oldbalanceDest:Float\",\n",
    "    \"newbalanceDest\" AS \"newbalanceDest:Float\",\n",
    "    \"isFraud\" AS \"isFraud:Int\",\n",
    "    \"isFlaggedFraud\" AS \"isFlaggedFraud:Int\"\n",
    "FROM {s3_tables_tablename}\n",
    "WHERE \"nameOrig\" IS NOT NULL AND \"nameDest\" IS NOT NULL\"\"\"\n",
    "\n",
    "await session.import_from_table(\n",
    "    graph[\"id\"],\n",
    "    s3_location_import,\n",
    "    [SOURCE_AND_DESTINATION_BANK_CUSTOMERS, BANK_TRANSACTIONS],\n",
    "    catalog=s3_tables_catalog,\n",
    "    database=s3_tables_database\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Importing to graph g-ijeab5wbj3\n",
      "INFO - Creating table using statement:\n",
      "SELECT DISTINCT \"~id\", 'customer' AS \"~label\"\n",
      "FROM (\n",
      "     SELECT \"nameOrig\" as \"~id\"\n",
      "     FROM transactions\n",
      "     WHERE \"nameOrig\" IS NOT NULL\n",
      "     UNION ALL\n",
      "     SELECT \"nameDest\" as \"~id\"\n",
      "     FROM transactions\n",
      "     WHERE \"nameDest\" IS NOT NULL\n",
      ")\n",
      "INFO - Executing query: 2dc7d5e2-9e7a-4f5b-b17c-cf3370202458\n",
      "INFO - Creating table using statement:\n",
      "SELECT\n",
      "    \"nameOrig\" as \"~from\",\n",
      "    \"nameDest\" as \"~to\",\n",
      "    \"type\" AS \"~label\",\n",
      "    \"step\" AS \"step:Int\",\n",
      "    \"amount\" AS \"amount:Float\",\n",
      "    \"oldbalanceOrg\" AS \"oldbalanceOrg:Float\",\n",
      "    \"newbalanceOrig\" AS \"newbalanceOrig:Float\",\n",
      "    \"oldbalanceDest\" AS \"oldbalanceDest:Float\",\n",
      "    \"newbalanceDest\" AS \"newbalanceDest:Float\",\n",
      "    \"isFraud\" AS \"isFraud:Int\",\n",
      "    \"isFlaggedFraud\" AS \"isFlaggedFraud:Int\"\n",
      "FROM transactions\n",
      "WHERE \"nameOrig\" IS NOT NULL AND \"nameDest\" IS NOT NULL\n",
      "INFO - Executing query: 4656646e-e5b7-4bc4-a186-e8439c539884\n",
      "INFO - [2026-02-04 16:10:41] Task [2dc7d5e2-9e7a-4f5b-b17c-cf3370202458] Current status: RUNNING\n",
      "INFO - [2026-02-04 16:10:41] Task [4656646e-e5b7-4bc4-a186-e8439c539884] Current status: QUEUED\n",
      "INFO - [2026-02-04 16:11:11] Task [2dc7d5e2-9e7a-4f5b-b17c-cf3370202458] Current status: SUCCEEDED\n",
      "INFO - Task [2dc7d5e2-9e7a-4f5b-b17c-cf3370202458] completed at [2026-02-04 16:11:11]\n",
      "INFO - [2026-02-04 16:11:11] Task [4656646e-e5b7-4bc4-a186-e8439c539884] Current status: SUCCEEDED\n",
      "INFO - Task [4656646e-e5b7-4bc4-a186-e8439c539884] completed at [2026-02-04 16:11:11]\n",
      "INFO - Confirmed CSV file exists: csv-import/2dc7d5e2-9e7a-4f5b-b17c-cf3370202458.csv\n",
      "INFO - Deleted metadata file: csv-import/2dc7d5e2-9e7a-4f5b-b17c-cf3370202458.csv.metadata\n",
      "INFO - Confirmed CSV file exists: csv-import/4656646e-e5b7-4bc4-a186-e8439c539884.csv\n",
      "INFO - Deleted metadata file: csv-import/4656646e-e5b7-4bc4-a186-e8439c539884.csv.metadata\n",
      "INFO - Successfully completed execution of 2 queries\n",
      "INFO - Created projection data in s3://nx-cit-patents/csv-import/\n",
      "INFO - [2026-02-04 16:11:14] Task [t-doljbxh515] Current status: INITIALIZING\n",
      "INFO - [2026-02-04 16:11:44] Task [t-doljbxh515] Current status: INITIALIZING\n",
      "INFO - [2026-02-04 16:12:14] Task [t-doljbxh515] Current status: IMPORTING\n",
      "INFO - [2026-02-04 16:12:44] Task [t-doljbxh515] Current status: IMPORTING\n",
      "INFO - [2026-02-04 16:13:14] Task [t-doljbxh515] Current status: IMPORTING\n",
      "INFO - [2026-02-04 16:13:44] Task [t-doljbxh515] Current status: IMPORTING\n",
      "INFO - [2026-02-04 16:14:14] Task [t-doljbxh515] Current status: IMPORTING\n",
      "INFO - [2026-02-04 16:14:45] Task [t-doljbxh515] Current status: FAILED\n",
      "ERROR - Unexpected status: FAILED on type: TaskType.IMPORT\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (UnexpectedStatus) when calling the wait_until_complete operation: Unexpected status",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mClientError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m      1\u001B[39m SOURCE_AND_DESTINATION_BANK_CUSTOMERS = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[33mSELECT DISTINCT \u001B[39m\u001B[33m\"\u001B[39m\u001B[33m~id\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m\u001B[33mcustomer\u001B[39m\u001B[33m'\u001B[39m\u001B[33m AS \u001B[39m\u001B[33m\"\u001B[39m\u001B[33m~label\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[33mFROM (\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     10\u001B[39m \u001B[33m     WHERE \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mnameDest\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m IS NOT NULL\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[33m)\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m     13\u001B[39m BANK_TRANSACTIONS = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[33mSELECT\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[33m    \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mnameOrig\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m as \u001B[39m\u001B[33m\"\u001B[39m\u001B[33m~from\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m,\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     26\u001B[39m \u001B[33mFROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms3_tables_tablename\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[32m     27\u001B[39m \u001B[33mWHERE \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mnameOrig\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m IS NOT NULL AND \u001B[39m\u001B[33m\"\u001B[39m\u001B[33mnameDest\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m IS NOT NULL\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m session.import_from_table(\n\u001B[32m     30\u001B[39m     graph[\u001B[33m\"\u001B[39m\u001B[33mid\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m     31\u001B[39m     s3_location_import,\n\u001B[32m     32\u001B[39m     [SOURCE_AND_DESTINATION_BANK_CUSTOMERS, BANK_TRANSACTIONS],\n\u001B[32m     33\u001B[39m     sql_parameters=[\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m],\n\u001B[32m     34\u001B[39m     catalog=s3_tables_catalog,\n\u001B[32m     35\u001B[39m     database=s3_tables_database\n\u001B[32m     36\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/git/nx-neptune/nx_neptune/session_manager.py:450\u001B[39m, in \u001B[36mSessionManager.import_from_table\u001B[39m\u001B[34m(self, graph, s3_location, sql_queries, sql_parameters, catalog, database, remove_buckets)\u001B[39m\n\u001B[32m    447\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCreated projection data in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms3_location\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    449\u001B[39m \u001B[38;5;66;03m# import the S3 CSV files to Neptune\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m450\u001B[39m task_id = \u001B[38;5;28;01mawait\u001B[39;00m instance_management.import_csv_from_s3(\n\u001B[32m    451\u001B[39m     NeptuneGraph(\n\u001B[32m    452\u001B[39m         NeptuneAnalyticsClient(graph_id, \u001B[38;5;28mself\u001B[39m._neptune_client),\n\u001B[32m    453\u001B[39m         IamClient(\u001B[38;5;28mself\u001B[39m._s3_iam_role, \u001B[38;5;28mself\u001B[39m._iam_client),\n\u001B[32m    454\u001B[39m         nx.Graph(),\n\u001B[32m    455\u001B[39m     ),\n\u001B[32m    456\u001B[39m     s3_location,\n\u001B[32m    457\u001B[39m     reset_graph_ahead,\n\u001B[32m    458\u001B[39m     skip_snapshot,\n\u001B[32m    459\u001B[39m )\n\u001B[32m    461\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m remove_buckets:\n\u001B[32m    462\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m query_execution_id \u001B[38;5;129;01min\u001B[39;00m query_execution_ids:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/git/nx-neptune/nx_neptune/instance_management.py:580\u001B[39m, in \u001B[36mimport_csv_from_s3\u001B[39m\u001B[34m(na_graph, s3_arn, reset_graph_ahead, skip_snapshot, polling_interval, max_attempts)\u001B[39m\n\u001B[32m    578\u001B[39m future = TaskFuture(task_id, TaskType.IMPORT, polling_interval, max_attempts)\n\u001B[32m    579\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m future.wait_until_complete(na_client)\n\u001B[32m--> \u001B[39m\u001B[32m580\u001B[39m \u001B[43mfuture\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    581\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m task_id\n",
      "\u001B[31mClientError\u001B[39m: An error occurred (UnexpectedStatus) when calling the wait_until_complete operation: Unexpected status"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Louvain Algorithm\n",
    "\n",
    "Create a NetworkX graph and initialize the connection to the Neptune Analytics instance.\n",
    "\n",
    "We will run the Louvain Community Detection Algorithm and mutate the graph storing the results of the vertex community in the \"community\" property\n",
    "\n",
    "You can see the results in the console by removing the `write_property` argument."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-04T23:53:04.591002Z",
     "start_time": "2026-02-04T23:53:01.729557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nx_neptune import NeptuneGraph, set_config_graph_id\n",
    "\n",
    "config = set_config_graph_id(graph[\"id\"])\n",
    "na_graph = NeptuneGraph.from_config(config)\n",
    "\n",
    "# sanity check: print out 10 vertices and edges from the Neptune Analytics graph\n",
    "ALL_NODES = \"MATCH (n) RETURN n LIMIT 10\"\n",
    "all_nodes = na_graph.execute_call(ALL_NODES)\n",
    "print(f\"all nodes: {all_nodes}\")\n",
    "\n",
    "ALL_EDGES = \"MATCH ()-[r]-() RETURN r LIMIT 10\"\n",
    "all_edges = na_graph.execute_call(ALL_EDGES)\n",
    "print(f\"all edges: {all_edges}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all nodes: [{'n': {'~id': 'C6858897', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C819722529', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C445917584', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C1000487648', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C1034818515', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C1971604125', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C489226678', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C1825456600', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C431051396', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}, {'n': {'~id': 'C203635252', '~entityType': 'node', '~labels': ['customer'], '~properties': {}}}]\n",
      "all edges: []\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.config.backends.neptune.graph_id = graph[\"id\"]\n",
    "\n",
    "# using Neptune Analytics, run the Louvain Community Detection Algorithm and mutate\n",
    "# the graph storing the results of the vertex community in the \"community\" property\n",
    "result = nx.community.louvain_communities(nx.Graph(), backend=\"neptune\", write_property=\"community\")\n",
    "print(f\"louvain result: \\n{result}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Neptune Analytics data and add it to S3 Tables as an Iceberg table\n",
    "\n",
    "Export the Neptune Analytics graph and a CSV export, and convert it to Iceberg format.  Use Athena to add it to S3 Tables Bucket."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for the CSV table\n",
    "csv_catalog = 'AwsDataCatalog'\n",
    "csv_database = 'bank_fraud_full'\n",
    "csv_table_name = 'transactions_csv'\n",
    "\n",
    "# for the iceberg table\n",
    "iceberg_vertices_table_name = 'customers_updated'\n",
    "iceberg_edges_table_name = 'transactions_updated'\n",
    "iceberg_catalog = 's3tablescatalog/nx-fraud-detection-data'\n",
    "iceberg_database = 'bank_fraud_full'\n",
    "\n",
    "await session.export_to_table(\n",
    "    graph[\"id\"],\n",
    "    s3_location_export,\n",
    "    csv_table_name,\n",
    "    csv_catalog,\n",
    "    csv_database,\n",
    "    iceberg_vertices_table_name,\n",
    "    iceberg_edges_table_name,\n",
    "    iceberg_catalog,\n",
    "    iceberg_database\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# destroy the session graphs\n",
    "session.destroy_all_graphs()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete lifecycle of running analytics from a datalake projection into Neptune Analytics instance:\n",
    "\n",
    "1. **Creation**: We created a new Neptune Analytics instance on demand\n",
    "2. **Import**: We imported a projection of the datalake\n",
    "3. **Usage**: We ran graph algorithms (Louvain) on the instance and mutated the data\n",
    "4. **Deletion**: We exported the updated data back into the datalake into an iceberg table\n",
    "\n",
    "The session manager (`SessionManager`) provides an easy mechanism to execute general datalake functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
