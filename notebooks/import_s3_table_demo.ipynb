{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune Analytics Instance Management With S3 Table Projections\n",
    "\n",
    "This notebook uses the SessionManager to create projections from S3 Table datalake, load the projection into Neptune Analytics through S3. We will use the Louvain algorithm to find potential fraudulent nodes, and export the mutated graph back into S3 for our datalake.\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Create a projection from S3 Tables bucket.\n",
    "2. Import the projection into Neptune Analytics.\n",
    "3. Run Louvain algorithm on the provisioned instance to create communities.\n",
    "4. Export the graph back into S3 Tables bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the necessary libraries and set up logging."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "\n",
    "import kagglehub\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from nx_neptune.session_manager import SessionManager"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configure logging to see detailed information about the instance creation process\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout  # Explicitly set output to stdout\n",
    ")\n",
    "# Enable debug logging for the instance management module\n",
    "for logger_name in [\n",
    "    'nx_neptune.instance_management',\n",
    "    'nx_neptune.session_manager',\n",
    "]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Configuration\n",
    "\n",
    "Check for environment variables necessary for the notebook."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_env_vars(var_names):\n",
    "    values = {}\n",
    "    for var_name in var_names:\n",
    "        value = os.getenv(var_name)\n",
    "        if not value:\n",
    "            print(f\"Warning: Environment Variable {var_name} is not defined\")\n",
    "            print(f\"You can set it using: %env {var_name}=your-value\")\n",
    "        else:\n",
    "            print(f\"Using {var_name}: {value}\")\n",
    "        values[var_name] = value\n",
    "    return values\n",
    "    \n",
    "# Check for optional environment variables\n",
    "env_vars = check_env_vars([\n",
    "    'NETWORKX_S3_IMPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_EXPORT_BUCKET_PATH',\n",
    "    'NETWORKX_S3_TABLES_CATALOG',\n",
    "    'NETWORKX_S3_TABLES_DATABASE',\n",
    "    'NETWORKX_S3_TABLES_TABLENAME',\n",
    "])\n",
    "\n",
    "# Get environment variables\n",
    "s3_location_import = os.getenv('NETWORKX_S3_IMPORT_BUCKET_PATH')\n",
    "s3_location_export = os.getenv('NETWORKX_S3_EXPORT_BUCKET_PATH')\n",
    "s3_tables_catalog = os.getenv('NETWORKX_S3_TABLES_CATALOG')\n",
    "s3_tables_database = os.getenv('NETWORKX_S3_TABLES_DATABASE')\n",
    "s3_tables_tablename = os.getenv('NETWORKX_S3_TABLES_TABLENAME')\n",
    "session_name = \"nx-athena-test-full\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Setup\n",
    "\n",
    "PaySim data is available from [kaggle](https://www.kaggle.com/code/kartik2112/fraud-detection-on-paysim-dataset/input?select=PS_20174392719_1491204439457_log.csv).\n",
    "\n",
    "Data should be uploaded to an S3 bucket, and an athena table created for that bucket.\n",
    "\n",
    "The PaySim dataset includes a simulated mobile money dataset, that involves transactions between client actors and banks. We can use this dataset to detect fraudulent activities in the simulated data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paysim_s3_bucket = 'nx-fraud-detection'\n",
    "paysim_s3_bucket_path = 'data/'\n",
    "\n",
    "# Download the latest version of paysim data\n",
    "paysim_path = Path(kagglehub.dataset_download(\"ealaxi/paysim1\"))\n",
    "\n",
    "print(\"Path to paysim dataset files:\", paysim_path)\n",
    "\n",
    "# upload CSV to an S3 bucket if necessary\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for file_path in paysim_path.iterdir():\n",
    "    if file_path.is_file():\n",
    "        # check if the file already exists\n",
    "        object_list = s3_client.list_objects_v2(\n",
    "            Bucket=paysim_s3_bucket,\n",
    "            Prefix=f\"{paysim_s3_bucket_path}{file_path.name}\"\n",
    "        )\n",
    "        found_keys = object_list[\"KeyCount\"]\n",
    "        print (f\"found {found_keys} matching keys for {paysim_s3_bucket_path}{file_path.name}\")\n",
    "\n",
    "        if found_keys == 0:\n",
    "            print(f\"uploading: {file_path.name} to {paysim_s3_bucket_path}{file_path.name}\")\n",
    "            s3_client.upload_file(\n",
    "                str(file_path),\n",
    "                paysim_s3_bucket,\n",
    "                f\"{paysim_s3_bucket_path}{file_path.name}\"\n",
    "            )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _execute_create_table(stmt, catalog, database, s3_logs_location):\n",
    "    athena_client = boto3.client('athena')\n",
    "\n",
    "    # run athena query and wait for it to complete\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=stmt,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database,\n",
    "            'Catalog': catalog\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': s3_logs_location\",\n",
    "        }\n",
    "    )\n",
    "    query_execution_id = response[\"QueryExecutionId\"]\n",
    "    print(f\"Start CREATE TABLE with execution id: {query_execution_id}\")\n",
    "\n",
    "    response_status = 'QUEUED'\n",
    "    while response_status == 'QUEUED' or response_status == 'RUNNING':\n",
    "        sleep(1)\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        response_status = response[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        print(f\"status: {response_status}\")\n",
    "    if response_status == 'SUCCEEDED':\n",
    "        print(\"CREATE TABLE completed\")\n",
    "    else:\n",
    "        print(\"CREATE TABLE failed - please check logs\")\n",
    "\n",
    "# create CSV table from the uploaded data\n",
    "\n",
    "paysim_csv_tablename = 'transactions'\n",
    "paysim_csv_catalog = 'AWSDataCatalog'\n",
    "paysim_csv_database = 'bank_fraud'\n",
    "s3_bucket_location = f\"s3://{paysim_s3_bucket}/{paysim_s3_bucket_path}\"\n",
    "\n",
    "create_csv_table_stmt = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {paysim_csv_tablename} (\n",
    "    `step` int,\n",
    "    `type` string,\n",
    "    `amount` float,\n",
    "    `nameOrig` string,\n",
    "    `oldbalanceOrg` float,\n",
    "    `newbalanceOrig` float,\n",
    "    `nameDest` string,\n",
    "    `oldbalanceDest` float,\n",
    "    `newbalanceDest` float,\n",
    "    `isFraud` int,\n",
    "    `isFlaggedFraud` int\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "WITH SERDEPROPERTIES ('field.delim' = ',')\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '{s3_bucket_location}'\n",
    "TBLPROPERTIES ('classification' = 'csv', 'skip.header.line.count'='1');\n",
    "\"\"\"\n",
    "\n",
    "_execute_create_table(create_csv_table_stmt, paysim_csv_catalog, paysim_csv_database, f\"s3://{paysim_s3_bucket}\")\n",
    "\n",
    "# create ICEBERG S3 table from the CSV table\n",
    "\n",
    "create_s3_table_stmt = f\"\"\"\n",
    "CREATE TABLE {s3_tables_tablename}\n",
    "WITH (\n",
    "  table_type = 'ICEBERG',\n",
    "  is_external = false\n",
    ")\n",
    "AS SELECT * FROM \"{paysim_csv_catalog}\".\"{paysim_csv_database}\".\"{paysim_csv_tablename}\"\n",
    "\"\"\"\n",
    "\n",
    "_execute_create_table(create_s3_table_stmt, s3_tables_catalog, s3_tables_database, f\"s3://{paysim_s3_bucket}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a New/Get existing Neptune Analytics Instance\n",
    "\n",
    "Provision a new Neptune Analytics instance on demand, or retrieve an existing neptune-graph. Creating a new instance may take several minutes to complete."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "session = SessionManager.session(session_name)\n",
    "graph_list = session.list_graphs(with_details=False)\n",
    "print(\"The following graphs are available:\")\n",
    "pprint(graph_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "session = SessionManager.session(session_name)\n",
    "graph = await session.get_or_create_graph(config={\"provisionedMemory\": 32})\n",
    "print(\"Retrieved graph:\")\n",
    "pprint(graph)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data from S3\n",
    "\n",
    "Import data from S3 into the Neptune Analytics graph and wait for the operation to complete. <br>\n",
    "IAM permisisons required for import: <br>\n",
    " - s3:GetObject, kms:Decrypt, kms:GenerateDataKey, kms:DescribeKey"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SOURCE_AND_DESTINATION_BANK_CUSTOMERS = f\"\"\"\n",
    "SELECT DISTINCT \"~id\", 'customer' AS \"~label\"\n",
    "FROM (\n",
    "     SELECT \"nameOrig\" as \"~id\"\n",
    "     FROM {s3_tables_tablename}\n",
    "     WHERE \"nameOrig\" IS NOT NULL\n",
    "     UNION ALL\n",
    "     SELECT \"nameDest\" as \"~id\"\n",
    "     FROM {s3_tables_tablename}\n",
    "     WHERE \"nameDest\" IS NOT NULL\n",
    ");\"\"\"\n",
    "\n",
    "BANK_TRANSACTIONS = f\"\"\"\n",
    "SELECT\n",
    "    \"nameOrig\" as \"~from\",\n",
    "    \"nameDest\" as \"~to\",\n",
    "    \"type\" AS \"~label\",\n",
    "    \"step\" AS \"step:Int\",\n",
    "    \"amount\" AS \"amount:Float\",\n",
    "    \"oldbalanceOrg\" AS \"oldbalanceOrg:Float\",\n",
    "    \"newbalanceOrig\" AS \"newbalanceOrig:Float\",\n",
    "    \"oldbalanceDest\" AS \"oldbalanceDest:Float\",\n",
    "    \"newbalanceDest\" AS \"newbalanceDest:Float\",\n",
    "    \"isFraud\" AS \"isFraud:Int\",\n",
    "    \"isFlaggedFraud\" AS \"isFlaggedFraud:Int\"\n",
    "FROM {s3_tables_tablename}\n",
    "WHERE \"nameOrig\" IS NOT NULL AND \"nameDest\" IS NOT NULL\"\"\"\n",
    "\n",
    "await session.import_from_table(\n",
    "    graph[\"id\"],\n",
    "    s3_location_import,\n",
    "    [SOURCE_AND_DESTINATION_BANK_CUSTOMERS, BANK_TRANSACTIONS],\n",
    "    catalog=s3_tables_catalog,\n",
    "    database=s3_tables_database\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Louvain Algorithm\n",
    "\n",
    "Create a NetworkX graph and initialize the connection to the Neptune Analytics instance.\n",
    "\n",
    "We will run the Louvain Community Detection Algorithm and mutate the graph storing the results of the vertex community in the \"community\" property\n",
    "\n",
    "You can see the results in the console by removing the `write_property` argument."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nx_neptune import NeptuneGraph, set_config_graph_id\n",
    "\n",
    "config = set_config_graph_id(graph[\"id\"])\n",
    "na_graph = NeptuneGraph.from_config(config)\n",
    "\n",
    "# sanity check: print out 10 vertices and edges from the Neptune Analytics graph\n",
    "ALL_NODES = \"MATCH (n) RETURN n LIMIT 10\"\n",
    "all_nodes = na_graph.execute_call(ALL_NODES)\n",
    "print(f\"all nodes: {all_nodes}\")\n",
    "\n",
    "ALL_EDGES = \"MATCH ()-[r]-() RETURN r LIMIT 10\"\n",
    "all_edges = na_graph.execute_call(ALL_EDGES)\n",
    "print(f\"all edges: {all_edges}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx.config.backends.neptune.graph_id = graph[\"id\"]\n",
    "\n",
    "# using Neptune Analytics, run the Louvain Community Detection Algorithm and mutate\n",
    "# the graph storing the results of the vertex community in the \"community\" property\n",
    "result = nx.community.louvain_communities(nx.Graph(), backend=\"neptune\", write_property=\"community\")\n",
    "print(f\"louvain result: \\n{result}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Neptune Analytics data and add it to S3 Tables as an Iceberg table\n",
    "\n",
    "Export the Neptune Analytics graph and a CSV export, and convert it to Iceberg format.  Use Athena to add it to S3 Tables Bucket."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for the CSV table\n",
    "csv_catalog = 'AwsDataCatalog'\n",
    "csv_database = 'bank_fraud_full'\n",
    "csv_table_name = 'transactions_csv'\n",
    "\n",
    "# for the iceberg table\n",
    "iceberg_vertices_table_name = 'customers_updated'\n",
    "iceberg_edges_table_name = 'transactions_updated'\n",
    "iceberg_catalog = 's3tablescatalog/nx-fraud-detection-data'\n",
    "iceberg_database = 'bank_fraud_full'\n",
    "\n",
    "await session.export_to_table(\n",
    "    graph[\"id\"],\n",
    "    s3_location_export,\n",
    "    csv_table_name,\n",
    "    csv_catalog,\n",
    "    csv_database,\n",
    "    iceberg_vertices_table_name,\n",
    "    iceberg_edges_table_name,\n",
    "    iceberg_catalog,\n",
    "    iceberg_database\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# destroy the session graphs\n",
    "session.destroy_all_graphs()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete lifecycle of running analytics from a datalake projection into Neptune Analytics instance:\n",
    "\n",
    "1. **Creation**: We created a new Neptune Analytics instance on demand\n",
    "2. **Import**: We imported a projection of the datalake\n",
    "3. **Usage**: We ran graph algorithms (Louvain) on the instance and mutated the data\n",
    "4. **Deletion**: We exported the updated data back into the datalake into an iceberg table\n",
    "\n",
    "The session manager (`SessionManager`) provides an easy mechanism to execute general datalake functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
